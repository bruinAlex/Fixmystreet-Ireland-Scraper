{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawl & Scrape\n",
    "Useful tutorial: https://www.youtube.com/watch?v=XjNm9bazxn8&index=5&list=WL  \n",
    "\n",
    "Target site(s):\n",
    "http://fixmystreet.ie/reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting url.\n",
    "base_url = 'http://fixmystreet.ie/reports'\n",
    "start_url = 'http://fixmystreet.ie/reports'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def links(url, target_string, exclude_string=None):\n",
    "    \"\"\"Return a list of the top level links.\n",
    "    \n",
    "    Args:\n",
    "        url (str): URL to pull html links from.\n",
    "        target_string (str): String to look for.\n",
    "\n",
    "    Returns:\n",
    "        list: List containing html links.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    url = url\n",
    "    source_code = requests.get(url)\n",
    "    plain_text = source_code.text\n",
    "    soup = BeautifulSoup(plain_text, 'lxml') # Pull the raw html and store it as text in soup\n",
    "\n",
    "    links_list = []\n",
    "\n",
    "    for el in soup.find_all('td', {'class': 'title', 'style': 'text-align: left'}):\n",
    "        for link in el.find_all('a'):\n",
    "            try:\n",
    "                if exclude_string == None:\n",
    "                    if target_string in link.get('href'):\n",
    "                        links_list.append(link.get('href'))\n",
    "                elif exclude_string != None:\n",
    "                    if (target_string in link.get('href')) & (exclude_string not in link.get('href')):\n",
    "                        links_list.append(link.get('href'))\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    return links_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://fixmystreet.ie/reports/Carlow',\n",
       " 'http://fixmystreet.ie/reports/Cavan',\n",
       " 'http://fixmystreet.ie/reports/Clare',\n",
       " 'http://fixmystreet.ie/reports/Cork+City',\n",
       " 'http://fixmystreet.ie/reports/Cork+County',\n",
       " 'http://fixmystreet.ie/reports/Donegal',\n",
       " 'http://fixmystreet.ie/reports/Dublin+City',\n",
       " 'http://fixmystreet.ie/reports/D%C3%BAn+Laoghaire-Rathdown',\n",
       " 'http://fixmystreet.ie/reports/Fingal',\n",
       " 'http://fixmystreet.ie/reports/Galway+City',\n",
       " 'http://fixmystreet.ie/reports/Galway+County',\n",
       " 'http://fixmystreet.ie/reports/Kerry',\n",
       " 'http://fixmystreet.ie/reports/Kildare',\n",
       " 'http://fixmystreet.ie/reports/Kilkenny',\n",
       " 'http://fixmystreet.ie/reports/Laois',\n",
       " 'http://fixmystreet.ie/reports/Leitrim',\n",
       " 'http://fixmystreet.ie/reports/Limerick+City',\n",
       " 'http://fixmystreet.ie/reports/Limerick+County',\n",
       " 'http://fixmystreet.ie/reports/Longford',\n",
       " 'http://fixmystreet.ie/reports/Louth',\n",
       " 'http://fixmystreet.ie/reports/Mayo',\n",
       " 'http://fixmystreet.ie/reports/Meath',\n",
       " 'http://fixmystreet.ie/reports/Monaghan',\n",
       " 'http://fixmystreet.ie/reports/North+Tipperary',\n",
       " 'http://fixmystreet.ie/reports/Offaly',\n",
       " 'http://fixmystreet.ie/reports/Roscommon',\n",
       " 'http://fixmystreet.ie/reports/Sligo',\n",
       " 'http://fixmystreet.ie/reports/South+Dublin',\n",
       " 'http://fixmystreet.ie/reports/South+Tipperary',\n",
       " 'http://fixmystreet.ie/reports/Waterford+City',\n",
       " 'http://fixmystreet.ie/reports/Waterford+County',\n",
       " 'http://fixmystreet.ie/reports/Westmeath',\n",
       " 'http://fixmystreet.ie/reports/Wexford',\n",
       " 'http://fixmystreet.ie/reports/Wicklow']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links(start_url, '/reports/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraper(url):\n",
    "    \"\"\"Return a dictionary of info for the requested URL.\n",
    "    \n",
    "    Args:\n",
    "        url (str): URL to scrape.\n",
    "\n",
    "    Returns:\n",
    "        dict: Contains scraped ship info with key = ship name, and values as ship info.\n",
    "    \n",
    "    \"\"\"\n",
    "    info = {}\n",
    "    \n",
    "    source_code = requests.get(url)\n",
    "    plain_text = source_code.text\n",
    "    soup = BeautifulSoup(plain_text, 'lxml')\n",
    "    \n",
    "    id_num = re.search('(\\d+$)', url).group(1)\n",
    "    try:\n",
    "        title = soup.find('div', attrs={'class': 'problem-header cf'}).find('h1').get_text()\n",
    "    except:\n",
    "        title = None\n",
    "    \n",
    "    try:\n",
    "        category_raw = soup.find('div', attrs={'class': 'problem-header cf'}).find('em').get_text()\n",
    "        category = re.search('in the (.+) category', category_raw).group(1)\n",
    "    except:\n",
    "        category = None\n",
    "    \n",
    "    try:\n",
    "        comment_raw = soup.find('div', attrs={'class': 'problem-header cf'}).find_all('p')[2].get_text()\n",
    "        comment = re.search('\\\\n(.+)', comment_raw).group(1)\n",
    "    except:\n",
    "        comment = None\n",
    "        \n",
    "    info[id_num] = {'id': id_num,\n",
    "                    'title': title,\n",
    "                    'category': category,\n",
    "                    'comment': comment\n",
    "                   }\n",
    "\n",
    "    return info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Single Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(scraper('http://fixmystreet.ie/report/27192'), orient='index').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def links_ul(url, target_string, exclude_string=None):\n",
    "    \"\"\"Return a list of the second level links.\n",
    "    Recursively run if pagination found.\n",
    "    \n",
    "    Args:\n",
    "        url (str): URL to pull html links from.\n",
    "        target_string (str): String to look for.\n",
    "\n",
    "    Returns:\n",
    "        list: List containing html links.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    url = url\n",
    "    source_code = requests.get(url)\n",
    "    plain_text = source_code.text\n",
    "    soup = BeautifulSoup(plain_text, 'lxml') # Pull the raw html and store it as text in soup\n",
    "\n",
    "    # Parse soup and look for links.\n",
    "    links_list = []\n",
    "    \n",
    "    # Look for additional pages and recursively call.\n",
    "    try:\n",
    "        if soup.find('p', {'class': 'pagination'}).find('a', {'class': 'next'}):\n",
    "            links_list += links_ul(soup.find('p', {'class': 'pagination'}).find('a', {'class': 'next'}).get('href'), target_string, exclude_string)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    for ul_item in soup.find_all('ul', {'class': 'issue-list-a'}):\n",
    "        for link in ul_item.find_all('a'):\n",
    "            \n",
    "#             # TEST\n",
    "#             print(ul_item, link)\n",
    "            \n",
    "            try:\n",
    "                if exclude_string == None:\n",
    "                    if target_string in link.get('href'):\n",
    "                        links_list.append(link.get('href'))\n",
    "                elif exclude_string != None:\n",
    "                    if (target_string in link.get('href')) & (exclude_string not in link.get('href')):\n",
    "                        links_list.append(link.get('href'))\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "    return links_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed download of 78 records in 0.77 minutes!\n"
     ]
    }
   ],
   "source": [
    "# Main scraping loop.\n",
    "# Requires top_level_links above.\n",
    "\n",
    "info = {}\n",
    "count = 0\n",
    "start_time = time.time()\n",
    "\n",
    "top_level_links = links(start_url, 'reports/', '?')\n",
    "\n",
    "for top_link in top_level_links:\n",
    "    # Grab next level links.\n",
    "    second_level_links = links_ul(top_link, 'report/')\n",
    "    \n",
    "#     # TEST\n",
    "#     print(second_level_links)\n",
    "#     break\n",
    "    \n",
    "    # Go to each link.\n",
    "    for second_link in second_level_links:\n",
    "        scraped_info = scraper(second_link) # dict\n",
    "        info.update(scraped_info) # Merges dict\n",
    "        \n",
    "        # Take a break to not hammer the site.\n",
    "        count += 1\n",
    "        if count % 100 == 0:            \n",
    "            print(count)\n",
    "            print('{:.2f} min elapsed'.format((time.time() - start_time)/ 60))\n",
    "        time.sleep(1)\n",
    "\n",
    "\n",
    "print('Completed download of {} records in {:.2f} minutes!'.format(count, (time.time() - start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save to csv.\n",
    "current_datetime = datetime.datetime.now()\n",
    "output_name = 'fixmystreet_list_ie_' + current_datetime.strftime(\"%Y-%m-%d_%H-%M\") + '.csv'\n",
    "\n",
    "pd.DataFrame.from_dict(info, orient='index').reset_index(drop=True).to_csv(output_name, index_label='index')\n",
    "# pd.DataFrame.from_dict(scraper('http://www.fixmystreet.org.au/report/1399'), orient='index').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

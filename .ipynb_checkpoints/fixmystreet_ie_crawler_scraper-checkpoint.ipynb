{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawl & Scrape\n",
    "Useful tutorial: https://www.youtube.com/watch?v=XjNm9bazxn8&index=5&list=WL  \n",
    "\n",
    "Target site(s):\n",
    "http://fixmystreet.ie/reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting url.\n",
    "base_url = 'http://fixmystreet.ie/reports'\n",
    "start_url = 'http://fixmystreet.ie/reports'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def links(url, target_string, exclude_string=None):\n",
    "    \"\"\"Return a list of the top level links.\n",
    "    \n",
    "    Args:\n",
    "        url (str): URL to pull html links from.\n",
    "        target_string (str): String to look for.\n",
    "\n",
    "    Returns:\n",
    "        list: List containing html links.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    url = url\n",
    "    source_code = requests.get(url)\n",
    "    plain_text = source_code.text\n",
    "    soup = BeautifulSoup(plain_text, 'lxml') # Pull the raw html and store it as text in soup\n",
    "\n",
    "    links_list = []\n",
    "\n",
    "    for el in soup.find_all('td', {'class': 'title', 'style': 'text-align: left'}):\n",
    "        for link in el.find_all('a'):\n",
    "            try:\n",
    "                if exclude_string == None:\n",
    "                    if target_string in link.get('href'):\n",
    "                        links_list.append(link.get('href'))\n",
    "                elif exclude_string != None:\n",
    "                    if (target_string in link.get('href')) & (exclude_string not in link.get('href')):\n",
    "                        links_list.append(link.get('href'))\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    return links_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://fixmystreet.ie/reports/Carlow',\n",
       " 'http://fixmystreet.ie/reports/Cavan',\n",
       " 'http://fixmystreet.ie/reports/Clare',\n",
       " 'http://fixmystreet.ie/reports/Cork+City',\n",
       " 'http://fixmystreet.ie/reports/Cork+County',\n",
       " 'http://fixmystreet.ie/reports/Donegal',\n",
       " 'http://fixmystreet.ie/reports/Dublin+City',\n",
       " 'http://fixmystreet.ie/reports/D%C3%BAn+Laoghaire-Rathdown',\n",
       " 'http://fixmystreet.ie/reports/Fingal',\n",
       " 'http://fixmystreet.ie/reports/Galway+City',\n",
       " 'http://fixmystreet.ie/reports/Galway+County',\n",
       " 'http://fixmystreet.ie/reports/Kerry',\n",
       " 'http://fixmystreet.ie/reports/Kildare',\n",
       " 'http://fixmystreet.ie/reports/Kilkenny',\n",
       " 'http://fixmystreet.ie/reports/Laois',\n",
       " 'http://fixmystreet.ie/reports/Leitrim',\n",
       " 'http://fixmystreet.ie/reports/Limerick+City',\n",
       " 'http://fixmystreet.ie/reports/Limerick+County',\n",
       " 'http://fixmystreet.ie/reports/Longford',\n",
       " 'http://fixmystreet.ie/reports/Louth',\n",
       " 'http://fixmystreet.ie/reports/Mayo',\n",
       " 'http://fixmystreet.ie/reports/Meath',\n",
       " 'http://fixmystreet.ie/reports/Monaghan',\n",
       " 'http://fixmystreet.ie/reports/North+Tipperary',\n",
       " 'http://fixmystreet.ie/reports/Offaly',\n",
       " 'http://fixmystreet.ie/reports/Roscommon',\n",
       " 'http://fixmystreet.ie/reports/Sligo',\n",
       " 'http://fixmystreet.ie/reports/South+Dublin',\n",
       " 'http://fixmystreet.ie/reports/South+Tipperary',\n",
       " 'http://fixmystreet.ie/reports/Waterford+City',\n",
       " 'http://fixmystreet.ie/reports/Waterford+County',\n",
       " 'http://fixmystreet.ie/reports/Westmeath',\n",
       " 'http://fixmystreet.ie/reports/Wexford',\n",
       " 'http://fixmystreet.ie/reports/Wicklow']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links(start_url, '/reports/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraper(url):\n",
    "    \"\"\"Return a dictionary of info for the requested URL.\n",
    "    \n",
    "    Args:\n",
    "        url (str): URL to scrape.\n",
    "\n",
    "    Returns:\n",
    "        dict: Contains scraped ship info with key = ship name, and values as ship info.\n",
    "    \n",
    "    \"\"\"\n",
    "    info = {}\n",
    "    \n",
    "    source_code = requests.get(url)\n",
    "    plain_text = source_code.text\n",
    "    soup = BeautifulSoup(plain_text, 'lxml')\n",
    "    \n",
    "    id_num = re.search('(\\d+$)', url).group(1)\n",
    "    try:\n",
    "        title = soup.find('div', attrs={'class': 'problem-header cf'}).find('h1').get_text()\n",
    "    except:\n",
    "        title = None\n",
    "    \n",
    "    try:\n",
    "        category_raw = soup.find('div', attrs={'class': 'problem-header cf'}).find('em').get_text()\n",
    "        category = re.search('in the (.+) category', category_raw).group(1)\n",
    "    except:\n",
    "        category = None\n",
    "    \n",
    "    try:\n",
    "        comment_raw = soup.find('div', attrs={'class': 'problem-header cf'}).find_all('p')[2].get_text()\n",
    "        comment = re.search('\\\\n(.+)', comment_raw).group(1)\n",
    "    except:\n",
    "        comment = None\n",
    "        \n",
    "    info[id_num] = {'id': id_num,\n",
    "                    'title': title,\n",
    "                    'category': category,\n",
    "                    'comment': comment\n",
    "                   }\n",
    "\n",
    "    return info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Single Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>category</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27192</td>\n",
       "      <td>Trees and number of heavy branches down</td>\n",
       "      <td>Tree and Grass Maintenance</td>\n",
       "      <td>Due to the high winds last week there are a nu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                    title                    category  \\\n",
       "0  27192  Trees and number of heavy branches down  Tree and Grass Maintenance   \n",
       "\n",
       "                                             comment  \n",
       "0  Due to the high winds last week there are a nu...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_dict(scraper('http://fixmystreet.ie/report/27192'), orient='index').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def links_ul(url, target_string, exclude_string=None):\n",
    "    \"\"\"Return a list of the second level links.\n",
    "    Recursively run if pagination found.\n",
    "    \n",
    "    Args:\n",
    "        url (str): URL to pull html links from.\n",
    "        target_string (str): String to look for.\n",
    "\n",
    "    Returns:\n",
    "        list: List containing html links.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    url = url\n",
    "    source_code = requests.get(url)\n",
    "    plain_text = source_code.text\n",
    "    soup = BeautifulSoup(plain_text, 'lxml') # Pull the raw html and store it as text in soup\n",
    "\n",
    "    # Parse soup and look for links.\n",
    "    links_list = []\n",
    "    \n",
    "    # Look for additional pages and recursively call.\n",
    "    try:\n",
    "        if soup.find('p', {'class': 'pagination'}).find('a', {'class': 'next'}):\n",
    "            links_list += links_ul(soup.find('p', {'class': 'pagination'}).find('a', {'class': 'next'}).get('href'), target_string, exclude_string)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    for ul_item in soup.find_all('ul', {'class': 'issue-list-a'}):\n",
    "        for link in ul_item.find_all('a'):\n",
    "            \n",
    "#             # TEST\n",
    "#             print(ul_item, link)\n",
    "            \n",
    "            try:\n",
    "                if exclude_string == None:\n",
    "                    if target_string in link.get('href'):\n",
    "                        links_list.append(link.get('href'))\n",
    "                elif exclude_string != None:\n",
    "                    if (target_string in link.get('href')) & (exclude_string not in link.get('href')):\n",
    "                        links_list.append(link.get('href'))\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "    return links_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-540a96883a5b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtop_link\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtop_level_links\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m# Grab next level links.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0msecond_level_links\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinks_ul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtop_link\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'report/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m#     # TEST\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-36-23ca436e8828>\u001b[0m in \u001b[0;36mlinks_ul\u001b[1;34m(url, target_string, exclude_string)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;31m# Look for additional pages and recursively call.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'p'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'class'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'pagination'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'class'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'next'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[0mlinks_list\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mlinks_ul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'p'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'class'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'pagination'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'class'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'next'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'href'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_string\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexclude_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find'"
     ]
    }
   ],
   "source": [
    "# Main scraping loop.\n",
    "# Requires top_level_links above.\n",
    "\n",
    "info = {}\n",
    "count = 0\n",
    "start_time = time.time()\n",
    "\n",
    "top_level_links = links(start_url, 'reports/', '?')\n",
    "\n",
    "for top_link in top_level_links:\n",
    "    # Grab next level links.\n",
    "    second_level_links = links_ul(top_link, 'report/')\n",
    "    \n",
    "#     # TEST\n",
    "#     print(second_level_links)\n",
    "#     break\n",
    "    \n",
    "    # Go to each link.\n",
    "    for second_link in second_level_links:\n",
    "        scraped_info = scraper(second_link) # dict\n",
    "        info.update(scraped_info) # Merges dict\n",
    "        \n",
    "        # Take a break to not hammer the site.\n",
    "        count += 1\n",
    "        if count % 100 == 0:\n",
    "            print(count)\n",
    "            print('{:.2f} min elapsed'.format((time.time() - start_time)/ 60))\n",
    "        time.sleep(1)\n",
    "\n",
    "\n",
    "print('Completed download of {} records in {:.2f} minutes!'.format(count, (time.time() - start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save to csv.\n",
    "current_datetime = datetime.datetime.now()\n",
    "output_name = 'fixmystreet_list_au_' + current_datetime.strftime(\"%Y-%m-%d_%H-%M\") + '.csv'\n",
    "\n",
    "pd.DataFrame.from_dict(info, orient='index').reset_index(drop=True).to_csv(output_name, index_label='index')\n",
    "# pd.DataFrame.from_dict(scraper('http://www.fixmystreet.org.au/report/1399'), orient='index').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
